#!/usr/bin/env python3
"""
Independent LLM Manager - Each model manages itself independently
æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹ç®¡ç†ï¼Œäº’ä¸å¹²æ‰°
"""

import asyncio
import time
import os
from datetime import datetime
from typing import Dict, Any, Optional
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class IndependentModelManager:
    """ç‹¬ç«‹æ¨¡å‹ç®¡ç†å™¨ - æ¯ä¸ªæ¨¡å‹æœ‰è‡ªå·±çš„çŠ¶æ€å’Œæ—¶é—´å‘¨æœŸ"""
    
    def __init__(self, model_name: str, multi_llm_client, display_duration: int = 7):
        self.model_name = model_name
        self.client = multi_llm_client
        self.display_duration = display_duration  # ç»“æœæ˜¾ç¤ºæ—¶é—´ï¼ˆç§’ï¼‰
        
        # ç‹¬ç«‹çŠ¶æ€ç®¡ç†
        self.is_analyzing = False
        self.is_displaying = False
        self.last_analysis_time = 0
        self.current_result = None
        self.result_timestamp = None

        # åˆ›å»ºç»“æœå­˜å‚¨ç›®å½• - ä¿å­˜åˆ°TEP_control/RCA_Results
        backend_dir = Path(__file__).parent
        tep_control_dir = backend_dir.parent
        self.results_dir = tep_control_dir / "RCA_Results"
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºä»Šæ—¥çš„MDæ–‡ä»¶
        today = datetime.now().strftime("%m%d")
        if model_name == "mistral":
            self.log_file = self.results_dir / f"LocalLLM_{today}.md"
        elif model_name == "lmstudio":
            self.log_file = self.results_dir / f"LMStudio_{today}.md"
        elif model_name == "gemini":
            self.log_file = self.results_dir / f"Gemini_{today}.md"
        else:
            self.log_file = self.results_dir / f"{model_name}_{today}.md"
            
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        self._init_log_file()
    
    def _init_log_file(self):
        """åˆå§‹åŒ–MDæ—¥å¿—æ–‡ä»¶"""
        if not self.log_file.exists():
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write(f"# {self.model_name.upper()} Root Cause Analysis Log\n\n")
                f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d')}\n")
                f.write(f"**Model**: {self.model_name}\n\n")
                f.write("---\n\n")
    
    def can_accept_request(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¥å—æ–°çš„åˆ†æè¯·æ±‚"""
        return not (self.is_analyzing or self.is_displaying)
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            "model_name": self.model_name,
            "is_analyzing": self.is_analyzing,
            "is_displaying": self.is_displaying,
            "can_accept": self.can_accept_request(),
            "current_result": self.current_result,
            "result_timestamp": self.result_timestamp,
            "last_analysis_time": self.last_analysis_time
        }
    
    async def analyze_fault(self, system_message: str, user_prompt: str, fault_features: list = None) -> Dict[str, Any]:
        """ç‹¬ç«‹åˆ†ææ•…éšœ - ä¸å—å…¶ä»–æ¨¡å‹å½±å“"""
        
        if not self.can_accept_request():
            return {
                "status": "busy",
                "message": f"{self.model_name} is currently busy (analyzing or displaying results)",
                "can_retry_after": self._get_retry_time()
            }
        
        # å¼€å§‹åˆ†æ
        self.is_analyzing = True
        analysis_start_time = time.time()
        
        try:
            logger.info(f"ğŸ¤– {self.model_name} starting independent analysis...")
            
            # è°ƒç”¨å¯¹åº”çš„æ¨¡å‹
            if self.model_name == "mistral":
                response = await self.client._query_mistral(system_message, user_prompt)
            elif self.model_name == "gemini":
                response = await self.client._query_gemini(system_message, user_prompt)
            elif self.model_name == "lmstudio":
                response = await self.client._query_lmstudio(system_message, user_prompt)
            elif self.model_name == "anthropic":
                response = await self.client._query_claude(system_message, user_prompt)
            else:
                raise ValueError(f"Unknown model: {self.model_name}")
            
            analysis_end_time = time.time()
            analysis_duration = round(analysis_end_time - analysis_start_time, 2)
            
            # å‡†å¤‡ç»“æœ
            result = {
                "model_name": self.model_name,
                "response": response,
                "analysis_duration": analysis_duration,
                "timestamp": datetime.now().isoformat(),
                "status": "success"
            }
            
            # ä¿å­˜åˆ°MDæ–‡ä»¶
            self._save_to_md(result, fault_features)
            
            # è®¾ç½®æ˜¾ç¤ºçŠ¶æ€
            self.current_result = result
            self.result_timestamp = time.time()
            self.is_analyzing = False
            self.is_displaying = True
            
            # å¯åŠ¨æ˜¾ç¤ºè®¡æ—¶å™¨
            asyncio.create_task(self._display_timer())
            
            logger.info(f"âœ… {self.model_name} analysis completed in {analysis_duration}s")
            return result
            
        except Exception as e:
            self.is_analyzing = False
            error_result = {
                "model_name": self.model_name,
                "response": f"Error: {str(e)}",
                "analysis_duration": 0,
                "timestamp": datetime.now().isoformat(),
                "status": "error"
            }
            
            # ä¹Ÿä¿å­˜é”™è¯¯åˆ°MDæ–‡ä»¶
            self._save_to_md(error_result, fault_features)
            
            logger.error(f"âŒ {self.model_name} analysis failed: {str(e)}")
            return error_result
    
    def _save_to_md(self, result: Dict[str, Any], fault_features: list = None):
        """ä¿å­˜ç»“æœåˆ°MDæ–‡ä»¶"""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"## Analysis at {result['timestamp']}\n\n")
                f.write(f"**Duration**: {result['analysis_duration']}s\n")
                f.write(f"**Status**: {result['status']}\n")
                
                if fault_features:
                    f.write(f"**Top Features**: {', '.join(fault_features[:6])}\n")
                
                f.write(f"\n### Root Cause Analysis:\n\n")
                f.write(f"{result['response']}\n\n")
                f.write("---\n\n")
                
        except Exception as e:
            logger.error(f"Failed to save {self.model_name} result to MD: {str(e)}")
    
    async def _display_timer(self):
        """æ˜¾ç¤ºè®¡æ—¶å™¨ - æ§åˆ¶ç»“æœæ˜¾ç¤ºæ—¶é—´"""
        await asyncio.sleep(self.display_duration)
        self.is_displaying = False
        self.last_analysis_time = time.time()
        logger.info(f"ğŸ“± {self.model_name} display period ended, ready for next analysis")
    
    def _get_retry_time(self) -> float:
        """è·å–å¯ä»¥é‡è¯•çš„æ—¶é—´"""
        if self.is_analyzing:
            return 0  # åˆ†æä¸­ï¼Œæ— æ³•é¢„æµ‹å®Œæˆæ—¶é—´
        elif self.is_displaying and self.result_timestamp:
            remaining = self.display_duration - (time.time() - self.result_timestamp)
            return max(0, remaining)
        return 0
    
    def freeze_display(self):
        """å†»ç»“æ˜¾ç¤º - ç”¨æˆ·å¯ä»¥ä»”ç»†æŸ¥çœ‹ç»“æœ"""
        if self.is_displaying:
            self.is_displaying = "frozen"  # ç‰¹æ®ŠçŠ¶æ€
            logger.info(f"ğŸ§Š {self.model_name} display frozen by user")
    
    def unfreeze_display(self):
        """è§£å†»æ˜¾ç¤º"""
        if self.is_displaying == "frozen":
            self.is_displaying = False
            self.last_analysis_time = time.time()
            logger.info(f"ğŸ”¥ {self.model_name} display unfrozen, ready for next analysis")


class IndependentLLMSystem:
    """ç‹¬ç«‹LLMç³»ç»Ÿç®¡ç†å™¨"""
    
    def __init__(self, multi_llm_client):
        self.client = multi_llm_client
        self.managers: Dict[str, IndependentModelManager] = {}
        
        # ä¸ºæ¯ä¸ªå¯ç”¨çš„æ¨¡å‹åˆ›å»ºç‹¬ç«‹ç®¡ç†å™¨
        # LM Studio: ~30såˆ†æ + 10sæ˜¾ç¤º = 40så‘¨æœŸ
        # Gemini: ~20såˆ†æ + 7sæ˜¾ç¤º = 27så‘¨æœŸ
        display_durations = {
            "mistral": 7,      # Local LLMæ˜¾ç¤º7ç§’ (å·²ç¦ç”¨)
            "lmstudio": 10,    # LM Studioæ˜¾ç¤º10ç§’
            "gemini": 7,       # Geminiæ˜¾ç¤º7ç§’
            "anthropic": 7
        }
        
        for model_name in self.client.enabled_models:
            duration = display_durations.get(model_name, 7)
            self.managers[model_name] = IndependentModelManager(
                model_name, self.client, duration
            )
            logger.info(f"ğŸ”§ Created independent manager for {model_name} (display: {duration}s)")
    
    async def analyze_with_model(self, model_name: str, system_message: str, user_prompt: str, fault_features: list = None) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œç‹¬ç«‹åˆ†æ"""
        if model_name not in self.managers:
            return {
                "status": "error",
                "message": f"Model {model_name} not available"
            }
        
        return await self.managers[model_name].analyze_fault(system_message, user_prompt, fault_features)
    
    def get_all_status(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¨¡å‹çš„çŠ¶æ€"""
        return {name: manager.get_status() for name, manager in self.managers.items()}
    
    def freeze_model_display(self, model_name: str):
        """å†»ç»“æŒ‡å®šæ¨¡å‹çš„æ˜¾ç¤º"""
        if model_name in self.managers:
            self.managers[model_name].freeze_display()
    
    def unfreeze_model_display(self, model_name: str):
        """è§£å†»æŒ‡å®šæ¨¡å‹çš„æ˜¾ç¤º"""
        if model_name in self.managers:
            self.managers[model_name].unfreeze_display()
